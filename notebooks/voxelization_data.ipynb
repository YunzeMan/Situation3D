{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description: This file is used to load the pretrained model and run the inference on the test set.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, glob, sys\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "import argparse\n",
    "import urllib\n",
    "import plyfile\n",
    "import torch\n",
    "from MinkowskiEngine import SparseTensor\n",
    "# add parent path of the current path to sys.path to import util\n",
    "sys.path.append('./Situation3D')\n",
    "from lib import config\n",
    "from models.mink_unet import DisNet\n",
    "import lib.openscene.feature_loader\n",
    "from lib.openscene.feature_loader import FusedFeatureLoader, collation_fn_eval_all\n",
    "import importlib\n",
    "\n",
    "cfg = config.load_cfg_from_cfg_file('./Situation3D/config/ours_openseg_pretrained.yaml')\n",
    "args = cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> len of the dataset:         2\n",
      "=> len of the loader:          1\n",
      "=> coords shape and examples:  torch.Size([165789, 4])\n",
      "=> feat shape and examples:    torch.Size([165789, 3])\n",
      "=> label shape and examples:   torch.Size([206895])\n",
      "=> feat_3d shape and examples: torch.Size([165789, 768])\n",
      "=> mask shape and examples:    torch.Size([165789])\n",
      "=> inds_reverse shape and examples:  torch.Size([206895])\n",
      "tensor([[  0, 415, 128, 128],\n",
      "        [  0, 415, 128,  93],\n",
      "        [  0, 415, 128,  95]], device='cuda:0', dtype=torch.int32)\n",
      "===============> END of BLOCK/1 <===============\n",
      "=> feat_bottleneck C&F shape and examples:  torch.Size([2357, 4]) tensor([0, 0, 0, 0], dtype=torch.int32) tensor([  1, 416, 432, 144], dtype=torch.int32)\n",
      "=> tensor_stride shape and examples:  [16, 16, 16]\n",
      "tensor([[  0, 400, 128,  32],\n",
      "        [  0, 400, 128,  48],\n",
      "        [  0, 400, 128,  80]], device='cuda:0', dtype=torch.int32)\n",
      "=> coords shape and examples:  torch.Size([1730, 3])\n",
      "=> unique_coords shape and examples:  torch.Size([514, 2])\n",
      "tensor([[  0, 224],\n",
      "        [  0, 240],\n",
      "        [  0, 256]], device='cuda:0', dtype=torch.int32)\n",
      "=> Actual 3D positions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1543717/1028627167.py:69: UserWarning: scatter_reduce() is in beta and the API may change at any time. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1550.)\n",
      "  reduced_feats = reduced_feats.scatter_reduce_(0, indices.unsqueeze(-1).expand_as(feats), feats, reduce='mean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1600, 4.6400],\n",
      "        [0.1600, 4.9600],\n",
      "        [0.1600, 5.2800]], device='cuda:0')\n",
      "=> reduced_feats shape and examples:  torch.Size([514, 256])\n",
      "=> coords shape and examples:  torch.Size([627, 3])\n",
      "=> unique_coords shape and examples:  torch.Size([298, 2])\n",
      "tensor([[  0, 176],\n",
      "        [  0, 192],\n",
      "        [  0, 224]], device='cuda:0', dtype=torch.int32)\n",
      "=> Actual 3D positions:\n",
      "tensor([[0.1600, 3.6800],\n",
      "        [0.1600, 4.0000],\n",
      "        [0.1600, 4.6400]], device='cuda:0')\n",
      "=> reduced_feats shape and examples:  torch.Size([298, 256])\n",
      "=> feat_layer5 C&F shape and examples:  torch.Size([9646, 4]) tensor([0, 0, 0, 0], dtype=torch.int32) tensor([  1, 416, 432, 152], dtype=torch.int32)\n",
      "tensor([[  0, 408, 128,  40],\n",
      "        [  0, 408, 128,  48],\n",
      "        [  0, 408, 128,  80]], device='cuda:0', dtype=torch.int32)\n",
      "=> feat_layer6 C&F shape and examples:  torch.Size([36247, 4]) tensor([0, 0, 0, 0], dtype=torch.int32) tensor([  1, 420, 436, 152], dtype=torch.int32)\n",
      "tensor([[  0, 412, 128,  92],\n",
      "        [  0, 412, 128, 104],\n",
      "        [  0, 412, 128, 120]], device='cuda:0', dtype=torch.int32)\n",
      "===============> END of CELL <===============\n"
     ]
    }
   ],
   "source": [
    "model = DisNet('openseg')\n",
    "val_data = FusedFeatureLoader(datapath_prefix=args.data_root,\n",
    "                            datapath_prefix_feat=args.data_root_2d_fused_feature,\n",
    "                            voxel_size=args.voxel_size, \n",
    "                            split=args.split, aug=False,\n",
    "                            memcache_init=args.use_shm, eval_all=True, identifier=6797,\n",
    "                            input_color=args.input_color)\n",
    "val_sampler = None\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=2,\n",
    "                                            shuffle=False, num_workers=args.test_workers, pin_memory=True,\n",
    "                                            drop_last=False, collate_fn=collation_fn_eval_all,\n",
    "                                            sampler=val_sampler)\n",
    "# # coords:       [A~<81369, 4] 79889 if no augmentation in voxelization\n",
    "# # feat:         [A~<81369, 3] all 1\n",
    "# # label:        [81369]\n",
    "# # feat_3d:      [A~<81369, 768]\n",
    "# # mask:         [A~<81369]\n",
    "# # inds_reverse: [81369]\n",
    "coords, feat, label, feat_3d, mask, inds_reverse = next(iter(val_loader))\n",
    "print('=> len of the dataset:        ', len(val_data))\n",
    "print('=> len of the loader:         ', len(val_loader))\n",
    "print('=> coords shape and examples: ', coords.shape)\n",
    "print('=> feat shape and examples:   ', feat.shape)\n",
    "print('=> label shape and examples:  ', label.shape)\n",
    "print('=> feat_3d shape and examples:', feat_3d.shape)\n",
    "print('=> mask shape and examples:   ', mask.shape)\n",
    "print('=> inds_reverse shape and examples: ', inds_reverse.shape)\n",
    "# print(coords[:3])\n",
    "# # From voxel to 3D coords\n",
    "# coords_3d = voxel / scale + voxel_size / 2\n",
    "\n",
    "# sinput.C:     [A~<81369, 3] looks just like input coords\n",
    "# sinput.F:     [A~<81369, 3] looks just like input feat\n",
    "sinput = SparseTensor(feat.cuda(non_blocking=True), coords.cuda(non_blocking=True))\n",
    "# print('=> sinput.C shape and examples: ', sinput.C.shape)\n",
    "print(sinput.C[:3])\n",
    "# print('=> sinput.F shape and examples: ', sinput.F.shape)\n",
    "# print(sinput.F[:3])\n",
    "\n",
    "# print(sinput.decomposed_coordinates[0].shape)\n",
    "# print(sinput.decomposed_coordinates[1].shape)\n",
    "\n",
    "print('===============> END of BLOCK/1 <===============')\n",
    "\n",
    "data_dict = {}\n",
    "data_dict['openscene_in'] = sinput\n",
    "model = model.cuda()\n",
    "checkpoint = torch.load('~/.cache/torch/hub/checkpoints/scannet_openseg.pth.tar')\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "data_dict = model(data_dict)\n",
    "\n",
    "feat_bottleneck = data_dict['feat_bottleneck']\n",
    "feat_layer5 = data_dict['feat_layer5']\n",
    "feat_layer6 = data_dict['feat_layer6']\n",
    "out = data_dict['openscene_out']\n",
    "print('=> feat_bottleneck C&F shape and examples: ', feat_bottleneck.C.shape, feat_bottleneck.C.min(0)[0].cpu(), feat_bottleneck.C.max(0)[0].cpu())\n",
    "print('=> tensor_stride shape and examples: ', feat_bottleneck.tensor_stride)\n",
    "print(feat_bottleneck.C[:3])\n",
    "# print(feat_bottleneck.F[:3])\n",
    "\n",
    "list_of_coords, list_of_featurs = feat_bottleneck.decomposed_coordinates_and_features\n",
    "for batch_idx, (coords, feats) in enumerate(zip(list_of_coords, list_of_featurs)):\n",
    "    print('=> coords shape and examples: ', coords.shape)\n",
    "    reduced_coords = coords[:, [0, 1]]\n",
    "    unique_coords, indices = reduced_coords.unique(dim=0, return_inverse=True)\n",
    "    reduced_feats = torch.zeros(unique_coords.size(0), feats.size(1), device=feats.device)\n",
    "    # sum / amax / mean\n",
    "    reduced_feats = reduced_feats.scatter_reduce_(0, indices.unsqueeze(-1).expand_as(feats), feats, reduce='mean')\n",
    "    print('=> unique_coords shape and examples: ', unique_coords.shape)\n",
    "    print(unique_coords[:3])\n",
    "    positions = (unique_coords + torch.tensor(feat_bottleneck.tensor_stride[0:2], device=feats.device)/2) * 0.02\n",
    "    print('=> Actual 3D positions:')\n",
    "    print(positions[:3])\n",
    "    print('=> reduced_feats shape and examples: ', reduced_feats.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# feat_bottleneck_dense, feat_bottleneck_min_coordinate, feat_bottleneck_tensor_stride = feat_bottleneck.dense()\n",
    "# print('=> feat_bottleneck_dense shape and examples: ', feat_bottleneck_dense.shape) # [2, 256, 27, 28, 10]\n",
    "# # print('=> min_coordinate shape and examples: ', feat_bottleneck_min_coordinate.shape)     # [0, 0, 0]\n",
    "# # print('=> tensor_stride shape and examples: ', feat_bottleneck_tensor_stride.shape)       # [16, 16, 16]\n",
    "# print(feat_bottleneck_dense[0, :, -1, -1, -1])\n",
    "# print(feat_bottleneck_dense[1, :, 8, 8, 0])\n",
    "    \n",
    "\n",
    "print('=> feat_layer5 C&F shape and examples: ', feat_layer5.C.shape, feat_layer5.C.min(0)[0].cpu(), feat_layer5.C.max(0)[0].cpu())\n",
    "print(feat_layer5.C[:3])\n",
    "# print(feat_layer5.F[:3])\n",
    "print('=> feat_layer6 C&F shape and examples: ', feat_layer6.C.shape, feat_layer6.C.min(0)[0].cpu(), feat_layer6.C.max(0)[0].cpu())\n",
    "print(feat_layer6.C[:3])\n",
    "# print(feat_layer6.F[:3])\n",
    "\n",
    "\n",
    "print('===============> END of CELL <===============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Examples of openscene processed data: locs_in  (81369, 3)\n",
      "[[0.5324214  4.5172734  0.26304942]\n",
      " [0.53404164 4.552089   0.262302  ]\n",
      " [0.544779   4.4811263  0.17396316]]\n",
      "=> Examples of openscene processed data: locs_in_homo  (81369, 4)\n",
      "[[1.         0.53242141 4.51727343 0.26304942]\n",
      " [1.         0.53404164 4.55208921 0.26230201]\n",
      " [1.         0.544779   4.48112631 0.17396316]]\n",
      "=> Examples of raw points:  (81369, 3)\n",
      "[[0.5324214  4.5172734  0.26304942]\n",
      " [0.53404164 4.552089   0.262302  ]\n",
      " [0.544779   4.4811263  0.17396316]]\n"
     ]
    }
   ],
   "source": [
    "# locs_in:      [81369, 3]      processed_data['feat']:         [19081, 768]\n",
    "# feats_in:     [81369, 3]      processed_data['mask_full']:    [81369]\n",
    "# labels_in:    [81369]\n",
    "locs_in, feats_in, labels_in = torch.load('./dataset/ScanNet/openscene/scannet_3d/train/scene0000_00_vh_clean_2.pth')\n",
    "processed_data = torch.load('./dataset/ScanNet/openscene/scannet_multiview_openseg/scene0000_00_0.pt')\n",
    "print('=> Examples of openscene processed data: locs_in ', locs_in.shape)\n",
    "print(locs_in[:3])\n",
    "locs_in_homo = np.concatenate((np.ones((locs_in.shape[0], 1), dtype=np.int64), locs_in), axis=1)\n",
    "print('=> Examples of openscene processed data: locs_in_homo ', locs_in_homo.shape)\n",
    "print(locs_in_homo[:3])\n",
    "\n",
    "# print('=> Examples of openscene processed data: labels_in ')\n",
    "# print(labels_in[:3])\n",
    "\n",
    "# # sqa_points: [50000, 9]\n",
    "# sqa_points = np.load('./dataset/sqa3d/SQA3D/ScanQA/data/scannet/scannet_data/scene0000_00_vert.npy')\n",
    "# sqa_points_aligned = np.load('./dataset/sqa3d/SQA3D/ScanQA/data/scannet/scannet_data/scene0000_00_aligned_vert.npy')\n",
    "\n",
    "# raw_points:   [81369, 3]\n",
    "raw_points_file = './dataset/sqa3d/SQA3D/ScanQA/data/scannet/scans/scene0000_00/scene0000_00_vh_clean_2.ply'\n",
    "a = plyfile.PlyData().read(raw_points_file)\n",
    "num_verts = a['vertex'].count\n",
    "raw_points = np.zeros(shape=[num_verts, 3], dtype=np.float32)\n",
    "raw_points[:,0] = a['vertex'].data['x']\n",
    "raw_points[:,1] = a['vertex'].data['y']\n",
    "raw_points[:,2] = a['vertex'].data['z']\n",
    "print('=> Examples of raw points: ', raw_points.shape)\n",
    "print(raw_points[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Examples of feats: \n",
      "tensor([[5.1000, 5.1000, 0.1000],\n",
      "        [5.1000, 5.1000, 1.1000],\n",
      "        [4.1000, 4.1000, 2.1000],\n",
      "        [4.1000, 4.1000, 3.1000],\n",
      "        [3.1000, 3.1000, 3.1000]])\n",
      "=> Examples of unique_coords: \n",
      "tensor([[3, 3],\n",
      "        [4, 4],\n",
      "        [5, 5]])\n",
      "=> Examples of reduced_feats: \n",
      "tensor([[ 3.1000,  3.1000,  3.1000],\n",
      "        [ 8.2000,  8.2000,  5.2000],\n",
      "        [10.2000, 10.2000,  1.2000]])\n"
     ]
    }
   ],
   "source": [
    "coords = torch.tensor([ [5, 5, 0], \n",
    "                        [5, 5, 1], \n",
    "                        [4, 4, 2], \n",
    "                        [4, 4, 3],\n",
    "                        [3, 3, 3]], dtype=torch.int64)\n",
    "feats = coords + 0.1\n",
    "print('=> Examples of feats: ')\n",
    "print(feats)\n",
    "\n",
    "# Remove the specified dimension from the coordinates\n",
    "reduced_coords = coords[:, [0, 1]]\n",
    "\n",
    "# Group by the reduced coordinates and sum the features\n",
    "unique_coords, indices = reduced_coords.unique(dim=0, return_inverse=True)\n",
    "\n",
    "reduced_feats = torch.zeros(unique_coords.size(0), feats.size(1), device=feats.device)\n",
    "reduced_feats = reduced_feats.scatter_add_(0, indices.unsqueeze(-1).expand_as(feats), feats)\n",
    "\n",
    "print('=> Examples of unique_coords: ')\n",
    "print(unique_coords)\n",
    "print('=> Examples of reduced_feats: ')\n",
    "print(reduced_feats)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
